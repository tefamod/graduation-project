{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EQ_R_xKiLz9D",
        "outputId": "64edc0d2-016c-45b2-faef-10bee7de2261"
      },
      "outputs": [],
      "source": [
        "#!pip install gym\n",
        "#!pip install pygame\n",
        "#!pip install gymnasium"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "psfvrMfGLz9a"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def generate_possible_states(bins):\n",
        "    \"\"\"\n",
        "    Generate all possible states in the environment by discretizing the state space.\n",
        "    \"\"\"\n",
        "    num_bins = [len(b) + 1 for b in bins]  # Number of bins for each dimension\n",
        "    possible_states = {}\n",
        "\n",
        "    for i in bins[0]:\n",
        "        for j in bins[1]:\n",
        "            for k in bins[2]:\n",
        "                for l in bins[3]:\n",
        "                    state = (i, j, k, l)\n",
        "                    possible_states[state] = -1  # Initialize with None action\n",
        "    return possible_states\n",
        "\n",
        "def find_nearest_state(observation):\n",
        "    \"\"\"\n",
        "    Find the nearest state in the possible states map based on the given observation.\n",
        "    \"\"\"\n",
        "    min_distance = float('inf')\n",
        "    nearest_state = None\n",
        "\n",
        "    for state in possible_states.keys():\n",
        "        distance = sum((observation[i] - state[i]) ** 2 for i in range(len(observation)))\n",
        "        if distance < min_distance:\n",
        "            min_distance = distance\n",
        "            nearest_state = state\n",
        "\n",
        "    return nearest_state\n",
        "\n",
        "def store_best_action(state, action):\n",
        "    \"\"\"\n",
        "    Store best action for a given state.\n",
        "    \"\"\"\n",
        "    possible_states[find_nearest_state(state)] = action\n",
        "\n",
        "# Define number of bins for each dimension of the observation space\n",
        "# Adjust these values based on the range of observations you observe during training/testing\n",
        "bins = [\n",
        "    np.linspace(-4.8000002e+00, 4.8000002e+00, 50),  # Cart position\n",
        "    np.linspace(-3.4028235e+38, 3.4028235e+38, 50),  # Cart velocity\n",
        "    np.linspace(-4.1887903e-01, 4.1887903e-01, 50),  # Pole angle\n",
        "    np.linspace(-3.4028235e+38, 3.4028235e+38, 50)  # Pole velocity at tip\n",
        "]\n",
        "\n",
        "# Generate all possible states\n",
        "possible_states = generate_possible_states(bins)\n",
        "\n",
        "# Example of storing best action for a state\n",
        "# Suppose 'state' is the current state tuple and 'best_action' is the best action for that state\n",
        "# store_best_action(state, best_action)\n",
        "\n",
        "# Accessing the best action for a given state\n",
        "# Suppose 'state' is the state tuple for which you want to access the best action\n",
        "# best_action = possible_states[state]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7MF8l8e4Lz9d",
        "outputId": "eeaeac10-2254-4bc9-de1e-2c628b236e37"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode number is 1\n",
            "Episode number is 2\n",
            "Episode number is 3\n",
            "Episode number is 4\n",
            "Episode number is 5\n",
            "Episode number is 6\n",
            "Episode number is 7\n",
            "Episode number is 8\n",
            "Episode number is 9\n",
            "Episode number is 10\n",
            "Episode number is 11\n",
            "Episode number is 12\n",
            "Episode number is 13\n",
            "Episode number is 14\n",
            "Episode number is 15\n",
            "Episode number is 16\n",
            "Episode number is 17\n",
            "Episode number is 18\n",
            "Episode number is 19\n",
            "Episode number is 20\n",
            "Episode number is 21\n",
            "Episode number is 22\n",
            "Episode number is 23\n",
            "Episode number is 24\n",
            "Episode number is 25\n"
          ]
        }
      ],
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "\n",
        "env = gym.make('CartPole-v1', render_mode=\"human\")\n",
        "obs, inf = env.reset()\n",
        "\n",
        "num_episodes = 25\n",
        "\n",
        "def run_episode(env, possible_states, Q):\n",
        "    episode = []\n",
        "    state, _ = env.reset()\n",
        "    done = False\n",
        "\n",
        "    while not done:\n",
        "        discretized_state = find_nearest_state(state)#discretize(state, bins)\n",
        "        action = possible_states[discretized_state]\n",
        "        if action == -1:\n",
        "            action = env.action_space.sample()  # Choose a random action if the state has not been visited yet\n",
        "        next_state, reward, done, _, _ = env.step(action)\n",
        "        episode.append((discretized_state, action, reward))\n",
        "        state = next_state\n",
        "\n",
        "    return episode\n",
        "\n",
        "def update_action_values(episode, Q, returns_sum, returns_count):\n",
        "    G = 0\n",
        "    for t in reversed(range(len(episode))):\n",
        "        state, action, reward = episode[t]\n",
        "        G += reward\n",
        "        if (state, action) not in [(x[0], x[1]) for x in episode[:t]]:\n",
        "            returns_sum[state][action] += G\n",
        "            returns_count[state][action] += 1\n",
        "            Q[state][action] = returns_sum[state][action] / returns_count[state][action]\n",
        "\n",
        "def improve_policy(Q, possible_states):\n",
        "    for state, actions in Q.items():\n",
        "        if state in possible_states:\n",
        "            possible_states[state] = np.argmax(actions)\n",
        "\n",
        "# Initialize action-value function Q(s, a) and returns counters\n",
        "Q = {state: [0] * env.action_space.n for state in possible_states.keys()}\n",
        "returns_sum = {state: [0] * env.action_space.n for state in possible_states.keys()}\n",
        "returns_count = {state: [0] * env.action_space.n for state in possible_states.keys()}\n",
        "\n",
        "# Run episodes and update Q-values\n",
        "for ep in range(num_episodes):\n",
        "    print(f\"Episode number is {ep+1}\")\n",
        "\n",
        "    episode = run_episode(env, possible_states, Q)\n",
        "    update_action_values(episode, Q, returns_sum, returns_count)\n",
        "\n",
        "# Improve policy based on Q-values\n",
        "improve_policy(Q, possible_states)\n",
        "\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3NFTKojXLz9j",
        "outputId": "b13fbc20-ec5e-4e5f-92c1-f19d77bae51c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pole angle at episode ends: -12.503689765930176\n",
            "reward in this episode: 10.0\n"
          ]
        }
      ],
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "\n",
        "env = gym.make('CartPole-v1', render_mode=\"human\")\n",
        "obs, inf = env.reset()\n",
        "env.render()\n",
        "\n",
        "total_reward = 0\n",
        "\n",
        "while True:\n",
        "    discretized_state = find_nearest_state(obs)#discretize(state, bins)\n",
        "    action = possible_states[discretized_state]\n",
        "    if action == -1:\n",
        "        action = env.action_space.sample()  # Choose a random action if the state has not been visited yet\n",
        "    obs, reward, terminated, truncated, info = env.step(action)\n",
        "    total_reward += reward\n",
        "    env.render()\n",
        "    if terminated or truncated:\n",
        "        print(f\"Pole angle at episode ends: {np.degrees(obs[2])}\", end=\"\\n\")\n",
        "        break\n",
        "print(f\"reward in this episode: {total_reward}\", end=\"\\n\")\n",
        "\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(-4.8000002, -6.944537755102045e+36, -0.41887903, -6.944537755102045e+36)"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "discretized_state = find_nearest_state(obs)\n",
        "discretized_state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "ename": "KeyError",
          "evalue": "(0.0, 0.0, -0.209439515, 0.0)",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mQ\u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m0.209439515\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n",
            "\u001b[1;31mKeyError\u001b[0m: (0.0, 0.0, -0.209439515, 0.0)"
          ]
        }
      ],
      "source": [
        "Q[(0.0, 0.0, -0.209439515, 0.0)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with 'Stable (Python 3.10.6)' requires the ipykernel package.\n",
            "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
            "\u001b[1;31mCommand: 'conda install -p c:\\Users\\User\\anaconda3\\envs\\Stable ipykernel --update-deps --force-reinstall'"
          ]
        }
      ],
      "source": [
        "returns_sum[(0.0, 0.0, -0.209439515, 0.0)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with 'Stable (Python 3.10.6)' requires the ipykernel package.\n",
            "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
            "\u001b[1;31mCommand: 'conda install -p c:\\Users\\User\\anaconda3\\envs\\Stable ipykernel --update-deps --force-reinstall'"
          ]
        }
      ],
      "source": [
        "possible_states[(0.0, 0.0, -0.209439515, 0.0)]"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
